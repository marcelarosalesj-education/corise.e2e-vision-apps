{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelarosalesj/e2e-vision-apps/blob/main/Week_2_Project_Self_Driving_Car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aktcbb3aRNw"
      },
      "source": [
        "#### This project is from [Abubakar Abid's](https://twitter.com/abidlabs) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/vision-applications)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJTIXeWUz_Gh"
      },
      "source": [
        "# Week 2 Project: Building the \"Eyes\" of a Self-Driving Car\n",
        "\n",
        "Welcome to the second week's project for *Building Computer Vision Applications*!\n",
        "\n",
        "In this week, we are going to get familiar with the key steps of machine learning, with a particular focus on image segmentation. Specifically, we will cover:\n",
        "\n",
        "* finding image segmentation datasets and pretrained models ðŸ“–\n",
        "* fine-tuning an image segmentation model on new data ðŸ‘¾\n",
        "* building a computer vision app you can run on your phone or laptop ðŸ“·\n",
        "* measuring the performance of a segmentation model on test data and the real world ðŸ“ˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaptPrn3sURb"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBXkJ-aIuZ4H"
      },
      "source": [
        "Self-driving cars are an exciting real-world application of machine learning, with the potential to save many lives each year. In order for self-driving cars to be fully autonomous, they need to \"see\" and \"understand\" the world around them. What are the machine learning algorithms that enable this? Let's take a look at [Tesla's website](https://www.tesla.com/AI): \"Our per-camera networks analyze raws images to perform **semantic segmentation**...\"\n",
        "\n",
        "What is semantic segmentation? Semantic segmentation is the process of assigning a class to *every pixel in an image*. In week 1, we studied *image classification*, which assigns a class to the entire image. Semantic segmentation is a more fine-grained version, which recognizes that an image can be made up of different objects: for example, an image taken by a camera on a self-driving car could consist of pedestrians, trees, and other cars. Semantic segmentation is used in many other applications as well, such as medical machine learning, where it can be used to identify organs in radiological images. Rather than assigning a single label to the entire image, a semantic segmentation model assigns each pixel a category so that we understand both *what* an image is, and *where* it is. \n",
        "\n",
        "By the end of this project, you'll have built an app that you can run on your laptop or phone that performs semantic segmentation on pictures of the outdoors scenes and will identify the road from the cars from the pedestrians, and so on. It will look something like this:\n",
        "\n",
        "![](https://i.ibb.co/RNv8MgQ/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37YHDIHJFUHw"
      },
      "source": [
        "# Step 0: Hardware Setup & Software Libraries\n",
        "\n",
        "We will be utilizing GPUs to train our machine learning model, so we will need to make sure that our Colab notebook is set up correctly. Go to the menu bar and click on Runtime > Change runtime type > Hardware accelerator and **make sure it is set to GPU**. Your Colab notebook may restart once you make the change.\n",
        "\n",
        "We're going to be using some fantastic open-source Python libraries to load our dataset (`datasets`), train our model (`transformers`), evaluate our model (`evaluate`), and build a demo of our model (`gradio`). So let's go ahead and install all of these libraries. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZoDxhyvfsS0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a00003-a909-411d-c920-36dcc730ec50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.7/dist-packages (0.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.7/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.7/dist-packages (from gradio) (0.23.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (from gradio) (0.85.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: paramiko in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.7/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.7/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from gradio) (3.15.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.7/dist-packages (from gradio) (10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (from gradio) (0.18.3)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.7/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: starlette==0.20.4 in /usr/local/lib/python3.7/dist-packages (from fastapi->gradio) (0.20.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.20.4->fastapi->gradio) (3.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi->gradio) (1.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (0.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.7/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (1.5.0)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (38.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers evaluate gradio huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Week 1, you created a Hugging Face account to upload your Gradio demo to Spaces. This week, we'll be uploading a model to your Hugging Face account *programmatically*! The first step is to log in using your Hugging Face token:"
      ],
      "metadata": {
        "id": "mcFlss5T9ByF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "qdp3asvm86kN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "e7yZ1Q__85v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "7903d17ba3db4699a9cb7f0ce98b7db8",
            "3b52bc7332bc4a3a8becfcc11c566434",
            "84f7cdacaed7433e9ca3344cf0b9c8ab",
            "141b0990735e444f83af8e72b33c61b2",
            "00ebf56507094497bf08d48d711a2041",
            "218b4107615647f5a45dd2329168850e",
            "23440d14eb1d491b9277628ac4b9a646",
            "8d2ad5d07e74460e90e0a748c17e7d23",
            "fc99c526fd0a4688a0d710cd8afc8300",
            "13988fa2088c41ca9039317818447467",
            "1d1edc232b994924beb669734ae2dbc2",
            "cdfcbe07e30f40b69578b9cef1277c72",
            "d5387cbe183249ffb937c021d522c59b",
            "aa7193207b714ccaa0e0a26ddecb86df"
          ]
        },
        "outputId": "ce72eee6-00b4-4838-9a15-15df43e38eef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5coFiTUWFfv",
        "outputId": "a8434772-6a47-4e96-e942-69ce4506120d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=\"CoRise-CV_applications_week_2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe8hTbPgWHKL",
        "outputId": "d92d1acf-9fdb-4bdb-f64b-073792f7f453"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=\"CoRise-CV_applications_week_2\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "WuWtNLYvWO2s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzyf35QUWRt5",
        "outputId": "1c8fbe51-e4b4-4c76-d7ed-3cee08b0c8d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcelarosalesj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qks6gJhucIC"
      },
      "source": [
        "# Step 1: Loading a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLiCkQlguhbD"
      },
      "source": [
        "In this project, we will be using the `datasets` library, which can load tens of thousands of datasets with a single line of code. It can also be used to apply preprocessing functions. Learn more about the datasets library here: https://huggingface.co/docs/datasets/tutorial\n",
        "\n",
        "Most datasets are divided into different splits. For example, you'll often see a *training* data subset, which is used to build the model, a *validation* data subset, which is used to measure the performance of the model while it is training, and a *test* dataset which is used to measure the performance of the model at the very end of training, and is usually considered to describe how well the model will perform in the real world (we'll come back to this).\n",
        "\n",
        "Specifically, we will be using the `segments/sidewalk-semantic` dataset that is available for free from the Hugging Face Hub: https://huggingface.co/datasets/segments/sidewalk-semantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8gImG59yeZ9"
      },
      "source": [
        "* **Load the Semantic Sidewalk Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RtYcR-Dx--o"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"segments/sidewalk-semantic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpvyH5NTymVY"
      },
      "source": [
        "* **Explore the dataset by running code below and reading the dataset card linked above. Answer the questions below**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRY7Xpmp_v7R"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of image: {dataset['train'][0]['pixel_values'].size}\")"
      ],
      "metadata": {
        "id": "UVoqOd1QNgvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  display(dataset['train'][i]['pixel_values'])"
      ],
      "metadata": {
        "id": "633KuSXZPmNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW4MX0mKzDCL"
      },
      "source": [
        "* How many training samples do we have? \n",
        " * 1000\n",
        "* What's the size of each image? \n",
        " * 1920 x 1080\n",
        "* How many categories are in this dataset's labels? \n",
        " * From the documentation, there are 35 categories.\n",
        "* Look at a random subset of ~10 training images, do you notice anything interesting about the images in the dataset? Are they as diverse/representative as you would expect or do they have limitations?\n",
        " * The images from the dataset are all from Belgium. Most of the roads are 1 lane, and there are not too many things happening on the roads.\n",
        " * In my opinion this dataset doesn't seem too diverse. Probably this could only work on Belgium (or a city with similar roads) with a few cars or people on the streets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7R8uW2Ho-p-"
      },
      "source": [
        "* **Simplifying the Training Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCclIrbQpDzy"
      },
      "source": [
        "You'll notice that the original dataset has many similar categories (for example, \"vehicle-car\" is a category, along with \"vehicle-truck\"). To simplify the training process, we will collapse together related categories. In the end, we will have 5 separate categories:\n",
        "* 0: road/sidewalk/path\n",
        "* 1: human\n",
        "* 2: vehicles\n",
        "* 3: other objects (e.g. traffic lights)\n",
        "* 4: nature and background\n",
        "\n",
        "For the purpose of this exercise, we will also make the images a lot smaller (64px by 64px) so that training is easier and faster. The following code processes the training images and labels.\n",
        "\n",
        "We've written the function that applies this transformation to a given sample. Efficiently apply it to each item in the dataset, using for example 8 CPU workers (even then, this code may take a few minutes to run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teZ3FtgD2KEg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "\n",
        "def transform(sample):\n",
        "    sample[\"pixel_values\"] = sample[\"pixel_values\"].convert(\"RGB\").resize((64,64))\n",
        "    sample[\"label\"] = sample[\"label\"].resize((64,64), Image.NEAREST)\n",
        "    collapse_categories = {**{i: 0 for i in range(1, 8)}, \n",
        "                            **{i: 1 for i in range(8, 10)}, \n",
        "                            **{i: 2 for i in range(10, 18)}, \n",
        "                            **{i: 3 for i in range(18, 28)}}\n",
        "    sample[\"label\"] = np.vectorize(lambda x: collapse_categories.get(x, 4))(np.array(sample[\"label\"]))\n",
        "    return sample\n",
        "    \n",
        "dataset = dataset.map(transform, num_proc=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5uoMm4rCGvy"
      },
      "source": [
        "Finally, shuffle the dataset and split the dataset into a training dataset (with 99% of the samples) and a test dataset (with the remaining 1%). We have a very small test dataset so that the evaluation step is quick. If you were training a model in a more realistic setting, you would pick a bigger evaluation dataset.\n",
        "\n",
        "You might find the `train_test_split()` method in the `datasets` library useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjMamZfU8K-b"
      },
      "outputs": [],
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.01, shuffle=True)\n",
        "\n",
        "train_ds = dataset[\"train\"]\n",
        "test_ds = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "WIWgCL4NZ2VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds"
      ],
      "metadata": {
        "id": "ufT9i9XfaPcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of samples\n",
        "train_ds[0]['pixel_values'].size"
      ],
      "metadata": {
        "id": "V5d3Z5GXaZLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s46qGGZBuqt"
      },
      "source": [
        "After you run the steps above, examine the `train_ds` and `test_ds` objects, and confirm that the samples look as you expect. Specifically,\n",
        "\n",
        "* How many training and test samples do we have?\n",
        " * training samples 990\n",
        " * test samples 10\n",
        "* What's the size of each image?\n",
        " * 64 x 64\n",
        "* What are the potential risks or downsides of having such a small test dataset?\n",
        " * The main risk of a small test dataset is not having enough data to evaluate for overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ0aT2Lkxu16"
      },
      "source": [
        "# Step 2: Loading a Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhROC7rh0OYQ"
      },
      "source": [
        "We will be using the `transformers` library, which can load tens of thousands of machine learning models with a few lines of code. It can also be used to fine-tune these models. Learn more about the `transformers` library here: https://huggingface.co/docs/transformers/index\n",
        "\n",
        "Specifically, we will be using the `Segformer` model that is available for anyone from the Hugging Face Hub: https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512. While the details of this architecture are beyond the scope of this course, we will point out that it is based on transformers, just like the vision transformers (ViT) network we used last week for image classification. Also, notice that it has already been fine-tuned for detecting everyday objects. We will _further_ fine-tune it for our specific dataset to speed up the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w11HNDrr1Ags"
      },
      "source": [
        "* **Load the Segformer Model and FeatureExtractor for Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKUBueAa07fl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "\n",
        "model.eval()\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NfyfEXLa1Et"
      },
      "source": [
        "We also need to load the **feature extractor** corresponding to the model, so that we can convert the input images into a feature vector that the model can take as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w37L3EPu3VxT"
      },
      "outputs": [],
      "source": [
        "extractor = AutoFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnbteoVGzZmw"
      },
      "source": [
        "# Step 3: Fine-tuning Your Model on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3a. Preprocess the Dataset and Load the Metric"
      ],
      "metadata": {
        "id": "M8UFMWhkj4WT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qD8r1v1oZO"
      },
      "source": [
        "Off the shelf, the Segformer model will not be usable for the task that we have in mind, since it was trained for \"general\" image segmentation, not for the specific categories that we would like to predict. As a result, we will need to \"fine-tune\" our model.\n",
        "\n",
        "Learn more about fine-tuning models with the `transformers` library here: https://huggingface.co/docs/transformers/training\n",
        "\n",
        "We will also need to decide which metric to use for our task. Since our task is image segmentation, the `mean IOU` metric seems reasonable: https://huggingface.co/spaces/evaluate-metric/mean_iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKAAlyqn5Onm"
      },
      "source": [
        "* **Preprocess the Dataset**\n",
        "\n",
        "We will convert the images to feature vectors on the fly as we train the model using the `set_transform()` method. This time, the `transform()` has been left for you to write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602EirtR5Q45"
      },
      "outputs": [],
      "source": [
        "def transform(example_batch):\n",
        "    images = [x for x in example_batch['pixel_values']]\n",
        "    labels = [x for x in example_batch['label']]\n",
        "    inputs = extractor(images, labels)\n",
        "    return inputs\n",
        "\n",
        "train_ds.set_transform(transform)\n",
        "test_ds.set_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]['pixel_values'].shape"
      ],
      "metadata": {
        "id": "ZB3zS7wogYwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "id": "pvmT1vfkQkPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_ds)"
      ],
      "metadata": {
        "id": "4j-QHH-PQmqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3b. Fine-Tune the Segformer Model on a Training Subset (and Overfit)"
      ],
      "metadata": {
        "id": "aqZFvYXKj_sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we discussed in lecture, a good way to start training a model is by making sure that you are able to overfit on a small subset of the training dataset. Train your model on 10 images from your training dataset for 10 epochs. \n",
        "\n",
        "We will start by defining our training hyperparameters as a `TrainingArguments` instance.\n",
        "\n",
        "Note that we leave the choice of learning rate to you. You may need to try different learning rates and batch sizes until you are able to overfit successfully on this training dataset.\n"
      ],
      "metadata": {
        "id": "Qq2jBx2dkHNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset_ds = train_ds.select(range(10))\n",
        "train_subset_ds.set_transform(transform)"
      ],
      "metadata": {
        "id": "cWsICdnOnAix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "lr = 0.0005\n",
        "epochs = 10\n",
        "batch_size = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"overfit-segmentation-model\",\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    eval_steps=5,\n",
        "    logging_strategy='epoch',\n",
        "    logging_steps=1,\n",
        "    report_to='wandb'\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset_ds,\n",
        "    eval_dataset=test_ds,\n",
        ")\n",
        "\n",
        "train_results = trainer.train()\n",
        "trainer.save_model(\"saved_model_files\")\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "hb0xJGTZlLL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Plot the Loss on the Training and Test Sets Over the 10 Epochs** "
      ],
      "metadata": {
        "id": "M3kuktq52ow7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(trainer.state.log_history)[['eval_loss', 'loss', 'epoch']]\n",
        "\n",
        "df_eval_loss = df[['eval_loss', 'epoch']].dropna()\n",
        "df_train_loss = df[['loss', 'epoch']].dropna()\n",
        "\n",
        "df_epoch = df_eval_loss.merge(df_train_loss, on='epoch', how='left')\n",
        "\n",
        "plt = df_epoch.plot.line(x='epoch', y=['loss', 'eval_loss'])"
      ],
      "metadata": {
        "id": "p8Vn3iGZQwoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bERJgeZNKh1j",
        "outputId": "3f6d6f1d-56f0-4c75-e2d8-7c2f93c522bd"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['loss', 'learning_rate', 'epoch', 'step', 'eval_loss', 'eval_runtime',\n",
              "       'eval_samples_per_second', 'eval_steps_per_second', 'train_runtime',\n",
              "       'train_samples_per_second', 'train_steps_per_second', 'total_flos',\n",
              "       'train_loss'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Is there any sign of overfitting? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "7R5-QhWO2twj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ0Qs3aK3Lf2"
      },
      "source": [
        "## 3c. Fine-Tune the Segformer Model on the Entire Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1Zet4oi8dm"
      },
      "source": [
        "* **Load the Mean IoU Metric**\n",
        "\n",
        "In addition to the loss, we now have to decide on a *metric* we will use to measure the performance for our machine learning model. A natural choice for image classification is *mean Intersection-over-Union (mean IoU)*, which measures the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth. It is probably the most common metric used for segmentation tasks. \n",
        "\n",
        "Read about the `evaluate` library, which contains many common machine learning metrics here: https://github.com/huggingface/evaluate\n",
        "\n",
        "And use `evaluate.load()` to load the mean IoU metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drhPTlyqi7mO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from torch import nn\n",
        "\n",
        "metric = # FILL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbmfg4eoXGQr"
      },
      "source": [
        "We will need to write some code to apply the mean IOU metric to the right layers of the neural network. We first need to convert our predictions to logits first, and then reshaped to match the size of the labels. This code has already been written for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P_XkpQAWXMM7"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    with torch.no_grad():\n",
        "        logits, labels = eval_pred\n",
        "        logits_tensor = torch.from_numpy(logits)\n",
        "        logits_tensor = nn.functional.interpolate(\n",
        "            logits_tensor,\n",
        "            size=labels.shape[-2:],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        ).argmax(dim=1)\n",
        "\n",
        "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
        "        metrics = metric.compute(\n",
        "            predictions=pred_labels,\n",
        "            references=labels,\n",
        "            num_labels=num_classes,\n",
        "            ignore_index=255,\n",
        "            reduce_labels=False,\n",
        "        )\n",
        "        for key, value in metrics.items():\n",
        "            if type(value) is np.ndarray:\n",
        "                metrics[key] = value.tolist()\n",
        "        return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH1xEP7klGcE"
      },
      "source": [
        "Now, we will take all of the code that you have written and use it to fine-tune the Segformer model on the sidewalk segmentation dataset. Simply run the code below, and your model will fine-tune for 5 epochs. On a **GPU**, this should take about or leass than 30 minutes with the default settings.\n",
        "\n",
        "**Important Note:** these default settings may **NOT** produce a very good segmentation model. For this task, you likely need significantly more training time. That is OK, the point of this exercise is not to train a highly-performant model, but to walk through the steps that would be needed to do that. We will **NOT** be looking at the performance of this model to grade your project. If you have been able to overfit on a small training subset (in part 3b), and the loss is going down in this part, that is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ukT6WLsu7wrv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b3c7c935-8200-454d-b582-9ccb7c122709"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ff599ca9e7dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"regular-segmentation-model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
          ]
        }
      ],
      "source": [
        "lr = 0.1\n",
        "epochs = 5\n",
        "batch_size = 1\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"regular-segmentation-model\",\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    logging_steps=20,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3d. Upload your model to the Hugging Face Hub!\n",
        "\n",
        "In two lines of code, upload your feature extractor and model to the Hugging Face Hub!"
      ],
      "metadata": {
        "id": "z3UWTsr76j8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.push_to_hub(\"my-segmentation-model\")"
      ],
      "metadata": {
        "id": "1CRL4KzGhXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"my-segmentation-model\")"
      ],
      "metadata": {
        "id": "ZlN3nE7P6jZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the URL for your model on the Hub? [ANSWER HERE]\n",
        "\n",
        "Please make sure that the model is **public**"
      ],
      "metadata": {
        "id": "kaf20N5H9v-a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckPFXvIszgKc"
      },
      "source": [
        "# Step 4: Reporting Model Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Plot the Loss and Mean IoU on the Training and Test Sets Over the 5 Epochs** "
      ],
      "metadata": {
        "id": "q3EyusBAgCKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL HERE"
      ],
      "metadata": {
        "id": "WyzF4IsJgNDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Is there any sign of overfitting? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "cRhVxDSdgUOk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPN5O5MczrbD"
      },
      "source": [
        "# Step 5: Building a Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBMiNYA_5W88"
      },
      "source": [
        "A high-level metric like mean test IoU doesn't give us a great idea on how the model will work when presented with new data from the real world. To understand this, we will build a web-based demo that we can use on our phones or computers through a web browser to test our model.\n",
        "\n",
        "The `gradio` library lets you build web demos of machine learning models with just a few lines code. Learn more about Gradio here: https://gradio.app/getting_started/\n",
        "\n",
        "Gradio lets you build machine learning demos simply by specifying (1) a prediction function, (2) the input type and (3) the output type of your model. We have already written the prediction function here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVq6jN8T6FjA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def classify(im):\n",
        "  inputs = extractor(images=im, return_tensors=\"pt\").to(\"cuda\")\n",
        "  outputs = model(**inputs)\n",
        "  logits = outputs.logits\n",
        "  classes = logits[0].detach().cpu().numpy().argmax(axis=0)\n",
        "  colors = np.array([[128,0,0], [128,128,0], [0, 0, 128], \t[128,0,128], [0, 0, 0]])\n",
        "  return colors[classes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhWOPJ2X57bY"
      },
      "source": [
        "* **Build a Gradio web demo of your image classifier and `launch()` it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMDMsc3KnS3U"
      },
      "source": [
        "Create a `gradio.Interface` and launch it! For image classification, the input component should be an `Image` component that passes the image in as a \"PIL\" image, and the output should be a `Image` component as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7ZKC4HEADGAG"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "interface = # FILL HERE\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH9_sZ_00JTB"
      },
      "source": [
        "# Step 6: Trying your Model with \"Real World\" Data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7K7GoRa6Fin"
      },
      "source": [
        "* **Use the share link created above to open up your app on your phone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ13z89G6juq"
      },
      "source": [
        "Now test your model on some real images -- perhaps you can go outside and take a picture of your car. Or you can upload a picture of a road you found online. Although your model may not have been trained for very long, is it still able to distinguish any object classes? Why do you think that may or may not be the case?  \n",
        "\n",
        "[ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Extensions\n",
        "\n",
        "Now that you've worked through the project and have a functioning app, what else can we try?\n",
        "* **Try training the model to convergence.** For this project, we only trained the model for 5 epochs, which is far too little for a real image segmentation model. Instead you can let the model train until it fully converges. How far can you increase the mean IoU?\n",
        "* **Try a zero-shot image segmentation model.** If you're tired of waiting for your model to train, you could try a zero-shot image segmentation models, which does not have to be retrained for specific applications. How well does a zero-shot segmentation model like [GroupViT](https://huggingface.co/nvidia/groupvit-gcc-yfcc) work for this problem?\n",
        "* **Set up an inference widget.** After you uploaded your model to the Hugging Face Hub, you may have noticed a message on the right side of the screen saying, \"Unable to determine this modelâ€™s pipeline type. Check the docs.\" This is usually where the inference widget goes, which allows anyone to try out your model directly from the web. Follow the docs to set up the inference widget for your model. \n",
        "* **Systematically explore different learning rates**: The learning rate is one of the most important hyperparameters when it comes to training machine learning models. Explore at least 8 different learning rates across 4 orders of magnitude. Which learning rates produce the best model?\n",
        "* **Try training a segmentation model on the original data**: To speed up the learning process, we reduced the number of classes and the resolution of the images. Can you successfully train a model on the original data? This might require you to have Colab Pro, so that you can fit the images in the original resolution in memory.\n"
      ],
      "metadata": {
        "id": "dYm6GXEFmox0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7f2n7MFbmA0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### This project is from [Abubakar Abid's](https://twitter.com/abidlabs) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/vision-applications)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7903d17ba3db4699a9cb7f0ce98b7db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b52bc7332bc4a3a8becfcc11c566434",
              "IPY_MODEL_84f7cdacaed7433e9ca3344cf0b9c8ab",
              "IPY_MODEL_141b0990735e444f83af8e72b33c61b2",
              "IPY_MODEL_00ebf56507094497bf08d48d711a2041"
            ],
            "layout": "IPY_MODEL_218b4107615647f5a45dd2329168850e"
          }
        },
        "3b52bc7332bc4a3a8becfcc11c566434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23440d14eb1d491b9277628ac4b9a646",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8d2ad5d07e74460e90e0a748c17e7d23",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "84f7cdacaed7433e9ca3344cf0b9c8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fc99c526fd0a4688a0d710cd8afc8300",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_13988fa2088c41ca9039317818447467",
            "value": ""
          }
        },
        "141b0990735e444f83af8e72b33c61b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1d1edc232b994924beb669734ae2dbc2",
            "style": "IPY_MODEL_cdfcbe07e30f40b69578b9cef1277c72",
            "tooltip": ""
          }
        },
        "00ebf56507094497bf08d48d711a2041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5387cbe183249ffb937c021d522c59b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aa7193207b714ccaa0e0a26ddecb86df",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "218b4107615647f5a45dd2329168850e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "23440d14eb1d491b9277628ac4b9a646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2ad5d07e74460e90e0a748c17e7d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc99c526fd0a4688a0d710cd8afc8300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13988fa2088c41ca9039317818447467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d1edc232b994924beb669734ae2dbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfcbe07e30f40b69578b9cef1277c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d5387cbe183249ffb937c021d522c59b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7193207b714ccaa0e0a26ddecb86df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}