{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelarosalesj/e2e-vision-apps/blob/main/Week_4_Project_Build_a_Movie_Poster_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aktcbb3aRNw"
      },
      "source": [
        "#### This project is from [Abubakar Abid's](https://twitter.com/abidlabs) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/vision-applications)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJTIXeWUz_Gh"
      },
      "source": [
        "# Week 4 Project: Building a Movie Poster Generator\n",
        "\n",
        "Welcome to the fourth week's project for *Building Computer Vision Applications*!\n",
        "\n",
        "In this final week, we are going to get familiar with the key steps of machine learning, with a particular focus on image generation. Specifically, we will cover:\n",
        "\n",
        "* finding pretrained image generation models from the Hugging Face Hub ðŸ‘¾\n",
        "* using models to generate specific kinds of images through prompt-engineering ðŸ“–\n",
        "* learning how to pipe machine learning models together to build more complex pipelines ðŸ”§\n",
        "* deploying the model as an app you can run on your phone or laptop ðŸ“·\n",
        "* collecting data from real-world usage of the app to further improve the model  ðŸ“ˆ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaptPrn3sURb"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBXkJ-aIuZ4H"
      },
      "source": [
        "[Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) is an open-source image generation model released in August 2022. The model uses machine learning to generate images from text prompts. It also has other uses, such as converting sketches into realistic images as well as learning new \"concepts\" to create custom images. Although other image generation models such as Midjourney and Dall-E exist, Stable Diffusion has the advantage of being completely open-source, while generating images of similar quality. Here is an example of the same prompt being fed into each of the three models:\n",
        "\n",
        "\n",
        "![](https://www.artificialintelligence-news.com/wp-content/uploads/sites/9/2022/08/stable-diffusion-text-to-image-ai-model-generator-stability.jpeg)\n",
        "\n",
        "\n",
        "Around the same time, Hugging Face released the `diffusers` library to make it very easy to work with Stable Diffusion as well as other models based on the same underlying \"diffusion\" algorithm. We will be using the `diffusers` library to generate images from the Stable Diffusion model. In particular, we will be generating movie posters that don't exist! By the end of the project, you will create an app that allows you put the name of a celebrity and the name of a movie and you'll produce a movie poster with them in it:\n",
        "\n",
        "![](https://i.ibb.co/QMPMsvz/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37YHDIHJFUHw"
      },
      "source": [
        "# Step 0: Hardware Setup & Software Libraries\n",
        "\n",
        "We will be utilizing GPUs to train our machine learning model, so we will need to make sure that our colab notebook is set up correctly. Go to the menu bar and click on Runtime > Change runtime type > Hardware accelerator and **make sure it is set to GPU**. Your colab notebook may restart once you make the change.\n",
        "\n",
        "We're going to be using several fantastic open-source Python libraries to load our model (`transformers` and `diffusers`) and to build a demo of our model (`gradio`). So let's go ahead and install all of these libraries. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoDxhyvfsS0N"
      },
      "outputs": [],
      "source": [
        "!pip install transformers huggingface_hub diffusers gradio "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qks6gJhucIC"
      },
      "source": [
        "# Step 1: Loading a Pretrained Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First, we'll load the pretrained Stable Diffusion model and use it to generate \"*a photo of an astronaut riding a horse on mars*\". In order to use Stable Diffusion, you first need to agree to the terms and conditions. First, make sure that you are logged into your Hugging Face account:"
      ],
      "metadata": {
        "id": "ou5chdFkxMcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "jtR6Wr09xLe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Then, go to [Stable Diffusion model card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree with the terms to use the model. Once you do that, you should be able to run the following lines of code to start generating images (might take a couple of minutes to download all of the model files the first time you run this):\n",
        "\n",
        "*Note*: We suggest only running the following cell *once*. Since the model is loaded into memory, you may run out of memory if you rerun this cell multipletimes."
      ],
      "metadata": {
        "id": "ujdSESVex5XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you're logged in with `huggingface-cli login`\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "image = pipe(prompt).images[0]  "
      ],
      "metadata": {
        "id": "cxlUOwoFx10u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see the image that you generated:"
      ],
      "metadata": {
        "id": "DFa1kEhYW62Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "5CPJEzDSxXHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate and display three more images with the same prompt."
      ],
      "metadata": {
        "id": "QX2xCbXxXqHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "ZdKreXf_X6sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What is the size of the resulting images? [ANSWER]\n",
        "\n",
        "* In what ways do the 4 images you've generated so far vary? In what ways do the stay the same? [ANSWER]\n"
      ],
      "metadata": {
        "id": "ldrL396sX2Ma"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7R8uW2Ho-p-"
      },
      "source": [
        "# Step 2: Generating Images with Diffusion Models\n",
        "\n",
        "Once we have our image generation model loaded, it's time to start experiment with it to understand it at a deeper level. As discussed in lecture and the reading, diffusion models generate images by \"diffusing\" noise iteratively, and there are **4 basic options** you can tweak in order to generate the kind of images you want\n",
        "\n",
        "* The original \"latent noise\" matrix that is converted into the image. This is controlled by the  `latents` parameter in `pipe()`. By fixing this, you can generate reproducible images, so this is also known as the *seed*.\n",
        "* The number of steps for which to denoise the image. This is controlled by the `num_inference_steps` parameter in `pipe()`. \n",
        "* The prompt itself: the most obvious thing to change is the prompt itself. This is more of an art than a science, but we recommend that you read some resources on how to design a good prompt, [such as this](https://www.howtogeek.com/833169/how-to-write-an-awesome-stable-diffusion-prompt/).\n",
        "* The guidance scale: this parameter controls how much you want the resulting image to be controlled by your prompt versus \"letting it do its own thing.\" This is controlled by the `guidance_scale` parameter in `pipe()`\n",
        "\n",
        "We'll explore the effect of these parameters in this step of the project. You might find it helpful to take a look at the parameters accepted by the [pipeline](https://github.com/huggingface/diffusers/blob/f3983d16eed57e46742d217363d8913bef7f748d/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L112).\n",
        "\n",
        "There is also (at least) **1 advanced option** that you can tweak in order to change the kinds of images\n",
        "\n",
        "* The \"scheduler\" which is the specific way that a noisy image is diffused or denoised. We'll explore this in the extensions.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a. Setting the random seeds\n",
        "\n",
        "First, we'll see how to set a random seed to control the latent noise matrix, which in turn controls the image that is generated. First, we will define a \"generator\" in PyTorch."
      ],
      "metadata": {
        "id": "ofQRN2Zwcfw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device)\n",
        "\n",
        "seed = generator.seed()\n",
        "print(f\"The seed for this generator is: {seed}\")"
      ],
      "metadata": {
        "id": "ih31cpvibp_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate a random Tensor that using the `generator` above to feed into the `latents` parameter. For the default Stable Diffusion pipeline,i t should be of size 1 x 4 x 64 x 64. Confirm that every time you generate this \"random\" generator, you get the same tensor.\n",
        "\n",
        "You might find `torch.randn()` and `torch.equal` helpful methods."
      ],
      "metadata": {
        "id": "mvrVs-vRfx0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "tf1ofLoQfwlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now generate two images of astronauts on horses using the same latent noise matrix you defined above, and confirm that they are the same:"
      ],
      "metadata": {
        "id": "WlR6WqK2hBfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "C2xWAjk5gIHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b. Setting the number of steps to generate the image\n",
        "\n",
        "Next up, let's see how the number of steps controls the quality of the generated image. By default, the `pipe()` method runs for 50 steps. What happens if you run it for 5, 10, 25, 50, and 100 steps?\n",
        "\n",
        "Generate images for each of these steps below ***using the same fixed latent noise*** \n"
      ],
      "metadata": {
        "id": "5dAvl_xphVDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "UCMYzM5ZhUW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the tradeoff that you experience when increasing the number of steps for which to run the pipeline? [ANSWER]\n"
      ],
      "metadata": {
        "id": "wBXG7PfVkWi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c. Choosing a Good Prompt\n",
        "\n",
        "Next up, we're going to experiment with different prompts, and understand how they affect the image that is generated by Stable Diffusion. Since there is no exact science in generating prompts, it is helpful to see what kinds of prompts tend to do well. Visit a website of Stable Diffusion-generated art, such as https://lexica.art/ and you'll notice that most good prompts have the following two-part structure:\n",
        "\n",
        "PROMPT = **Detailed Description of Object** + *Style Modifiers*\n",
        "\n",
        "Here are a couple of examples:\n",
        "\n",
        "* [**big window, mountains in background, cloud forest in background, tropical beach in background**, *sunset, warm golden hour lighting, holiday vibes, living room, furniture, IKEA catalogue, futuristic, ultra realistic, ultra detailed, cinematic light, anamorphic, wooden floored balcony, by Paul Lehr*](https://lexica.art/prompt/a8985a90-708a-4786-88cd-6dabad79737c)\n",
        "* [**portrait of a super cute bunny, a carrot**, *pixar, zootopia, cgi, blade runner. trending on artstation*](https://lexica.art/prompt/8d078a31-2414-44d7-bab7-aa02067af61e)\n",
        "\n",
        "Generally, the more details and style modifiers you provide, the better the final result.\n",
        "\n",
        "For this part, we'd like you to generate a **movie poster** that does NOT exist in real life. It should star a real celebrity in a fictional movie, TV show, or setting that they do NOT act in.\n",
        "\n",
        "You should start off with a simple prompt, such as: \"*A movie poster of Robert Downey Jr in Downton Abbey*\" (please choose a **different** celebrity and setting for your experiments). This might not get you a particularly realistic movie poster, so we'd like you to then add more details and stylistic modifiers to improve the generated images.\n",
        "\n",
        "Please display the results of at least 5 different prompts below and try to get the best movie poster you can!\n",
        "\n",
        "*Note*: you might end up with a completely black image if the \"safety filter\" of the StableDiffusion model has been triggered. In some cases, the \"safety filter\" might be too sensitive and flag an image even with a relatively safe prompt. Please try again with a slightly different prompt if you think that has happened. \n",
        "\n"
      ],
      "metadata": {
        "id": "FSWhQWXZlQCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "pCcdkm1plATm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2d. Choosing a Guidance Scale\n",
        "\n",
        "The last parameter we need to understand is the guidance scale. Choosing a guidance scale affects how closely Stable Diffusion will stick to the prompt that you provide. The default value is `7.5`, but try the following guidance scales: `3`, `7`, `12`, and `20` to understand the tradeoffs between choosing different values:"
      ],
      "metadata": {
        "id": "aNZSAQQfoRQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "_KfqbIpVpDvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When should you choose a lower value for the guidance scale? When should you choose a higher value? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "ljL-RQRcpG_M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ0aT2Lkxu16"
      },
      "source": [
        "# Step 3: Improving Images of Faces\n",
        "\n",
        "A well-known problem of ML-generated images is that faces and particularly *eyes* are usually not generated very realistically. We are going to solve this problem by... using MORE machine learning.\n",
        "\n",
        "In particular, we are going to use the [GFP-GAN](https://huggingface.co/spaces/akhaliq/GFPGAN), a machine learning model that is designed to restore enhance old portraits. As it turns out, the model can ALSO be used to improve the faces and eyes in ML-generated images. \n",
        "\n",
        "For example, here is the image that I generated with the prompt above: \"*A movie poster of Robert Downey Jr in Downton Abbey*\"\n",
        "\n",
        "![link text](https://i.ibb.co/qkCR6rp/bef.png)\n",
        "\n",
        "After passing it through GFP-GAN, the eyes and faces were rendered far more realistically. As a side benefit of using GFP-GAN, the *resolution* of the image is also increased!\n",
        "\n",
        "![link text](https://i.ibb.co/BZQ8bPq/aft.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how can you use GFP-GAN? Well one way would be to drag-and-drop your image into the Gradio demo here: https://huggingface.co/spaces/akhaliq/GFPGAN \n",
        "\n",
        "But instead, we'd like for you to use the demo *programmatically*! Every Gradio demo comes with an API that you can use to make requests to it programmatically. You can see the documentation of this API by clicking the \"view API\" button at the bototm of the demo:\n",
        "\n",
        "![](https://i.ibb.co/5BybtRt/image.png)"
      ],
      "metadata": {
        "id": "vpCYWzBArCJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For this step of the project, take the 5 images that you generated in Step 2, and pass them through the GFP-GAN demo programmatically. Display the original images alongside the \"enhanced\" images:\n",
        "\n",
        "(Note that if this demo has a long queue, you could use another version of the GFPGAN Space such as https://huggingface.co/spaces/NotFungibleIO/GFPGAN)\n",
        "\n",
        "*Hint*: we suggest using the `requests` library to make POST requests to the GFP-GAN demo and the `base64` library to convert images to and from base64 format."
      ],
      "metadata": {
        "id": "rvW0gXKQsZvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "\n",
        "def improve_image(img):\n",
        "  # ANSWER HERE"
      ],
      "metadata": {
        "id": "kHo99XTZsZaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What is the resolution of your original images? What is the resolution of the images after they have been processed by GFP-GAN? [ANSWER HERE]"
      ],
      "metadata": {
        "id": "GQvRLg2ttHrP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8hKSUcL7IvM"
      },
      "source": [
        "# Step 4: Building a Machine Learning Web App\n",
        "\n",
        "Now, we finally have all of the pieces to build our Gradio machine learning app. Build (and launch!) a Gradio app that accepts the following:\n",
        "\n",
        "* A textbox for a celebrity name\n",
        "* A dropdown with a list of movies, TV shows, or settings\n",
        "\n",
        "And produces the following output:\n",
        "\n",
        "* An image of a movie poster starring that celebrity in that movie/show/setting. \n",
        "\n",
        "Use the best prompt structure that you discovered in Step 2, and pass the image through GFP-GAN before returning it to the output, as in Step 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate(celebrity, setting):\n",
        "  # ANSWER HERE\n",
        "  return image\n",
        "\n",
        "gr.Interface(\n",
        "  # ANSWER HERE    \n",
        ")  "
      ],
      "metadata": {
        "id": "iwuVDIQVt9Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPN5O5MczrbD"
      },
      "source": [
        "# Step 5: Collecting Data to Improve the Model\n",
        "\n",
        "When trying out our demo, we might find that some celebrities or movies may not produce very realistic images. For example, this might happen with less famous celebrities if Stable Diffusion does not \"know\" enough about them. Or it could be signs of bias in the data, as discussed in Week 3. As a result, we may want users to be able to FLAG those prompts and save the resulting data in a HuggingFace Dataset so that we can improve the model's performance (this is explored further in the second extension, **Textual Inversion**).\n",
        "\n",
        "In this Step, we will adapt our Gradio demo from Step 4 to be able to save generated images. Please take a look at: https://gradio.app/using_flagging/#the-huggingfacedatasetsaver-callback and fill in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "import os\n",
        "\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "hf_writer = # ANSWER HERE\n",
        "\n",
        "gr.Interface(\n",
        "  # ANSWER HERE    \n",
        ")"
      ],
      "metadata": {
        "id": "KWcnI3acuzie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flag a few example images and ensure that they appear in your Hugging Face Dataset. \n",
        "\n",
        "* What is the URL to your dataset: [ANSWER HERE]\n",
        "\n",
        "Please make sure that the dataset is **public**"
      ],
      "metadata": {
        "id": "ccHvEZ1Sw5mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Extensions\n",
        "\n",
        "* **Schedulers**: The `diffusers` library includes support for different schedulers, [as described here](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers). Which other schedulers are compatible with the `StableDiffusion` model? Experiment with a few of these schdulers -- what tradeoffs do you notice between different schedulers?\n",
        "* **Textual Inversion**: There are some names or \"concepts\" that Stable Diffusion doesn't know about. For example, unless you are famous, Stable Diffusion may not know your name. Or the model might not know one of the celebrities in your training dataset from last week. You can \"teach\" StableDiffusion new concepts by uploading a few images using a technique called Textual Inversion. Teach Stable Diffusion either the name of a celebrity (you can use your dataset from last week) or your own name [using Textual Inversion](https://huggingface.co/docs/diffusers/training/text_inversion), and then display movie posters generated by the original Stable Diffusion versus your new version. \n",
        "* **Your own GFP-GAN Space**: If you find that all of the GFP-GAN demos on Spaces have a long queue, you might want to clone your own version of the GFP-GAN Space, and run it to get your own private demo. Do that (you might find this [repo duplicator](https://huggingface.co/spaces/osanseviero/repo_duplicator) useful) and then use it for your own API.\n",
        "* **Use a different model than StableDiffusion**: in this entire project, we used StableDiffusion. Try using a different diffusion model to generate images -- what tradeoffs do you notice with the other model?"
      ],
      "metadata": {
        "id": "1rYGD2TWOBYv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7f2n7MFbmA0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### This project is from [Abubakar Abid's](https://twitter.com/abidlabs) course: *Building Computer Vision Applications* on CoRise. Learn more about the course [here](https://corise.com/course/vision-applications)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}